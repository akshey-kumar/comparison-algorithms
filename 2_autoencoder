import sys
sys.path.append(r'../')
import numpy as np
from functions import *

algorithm = 'PCA'

### Load Data (and excluding behavioural neurons)
for worm_num in range(5):
  b_neurons = [
    'AVAR',
    'AVAL',
    'SMDVR',
    'SMDVL',
    'SMDDR',
    'SMDDL',
    'RIBR',
    'RIBL',]
  data = Database(data_set_no=worm_num)
  data.exclude_neurons(b_neurons)
  X = data.neuron_traces.T
  B = data.states
  state_names = ['Dorsal turn', 'Forward', 'No state', 'Reverse-1', 'Reverse-2', 'Sustained reversal', 'Slowing', 'Ventral turn']

  ### Preprocess and prepare data for BundLe Net
  time, X = preprocess_data(X, data.fps)
  X_, B_ = prep_data(X, B, win=15)

  ## Train test split 
  X_train, X_test, B_train_1, B_test_1 = timeseries_train_test_split(X_, B_)

  ### Deploy PCA
  dim = 3
  pca = PCA(n_components=dim)
  pca.fit(X_train[:,0,0,:])
  print('Percentage of variance explained by the first ', dim, ' PCs: ', pca.explained_variance_ratio_[:dim].sum().round(3))
  
  ### Projecting into latent space
  Y0_tr = pca.transform(X_train[:,0,0,:])
  Y1_tr = pca.transform(X_train[:,1,0,:])

  Y0_tst = pca.transform(X_test[:,0,0,:])
  Y1_tst = pca.transform(X_test[:,1,0,:])

  # Save the weights
  # model.save_weights('data/generated/BunDLeNet_model_worm_' + str(worm_num))
  np.savetxt('data/generated/saved_Y/Y0_tr__'+algorithm+'_worm_'+ str(worm_num), Y0_tr)
  np.savetxt('data/generated/saved_Y/Y1_tr__'+algorithm+'_worm_'+ str(worm_num), Y1_tr)
  np.savetxt('data/generated/saved_Y/Y0_tst__'+algorithm+'_worm_'+ str(worm_num), Y0_tst)
  np.savetxt('data/generated/saved_Y/Y1_tst__'+algorithm+'_worm_'+ str(worm_num), Y1_tst)
  np.savetxt('data/generated/saved_Y/B_train_1__'+algorithm+'_worm_'+ str(worm_num), B_train_1)
  np.savetxt('data/generated/saved_Y/B_test_1__'+algorithm+'_worm_'+ str(worm_num), B_test_1)










# Defining the model
class Autoencoder(Model):
  def __init__(self, latent_dim):
    super(Autoencoder, self).__init__()
    self.latent_dim = latent_dim   
    self.encoder = tf.keras.Sequential([
        layers.Flatten(),
        layers.Dense(100, activation='relu'),
        layers.Dense(50, activation='relu'),
        layers.Dense(10, activation='relu'),
        layers.Dense(latent_dim, activation='relu'),
    ])
    self.decoder = tf.keras.Sequential([
        layers.Dense(latent_dim, activation='relu'),
        layers.Dense(10, activation='relu'),
        layers.Dense(50, activation='relu'),
        layers.Dense(100, activation='relu'),
        layers.Dense(100, activation='relu'),
        layers.Dense(X0_tr.shape[-1]*X0_tr.shape[-2], activation='linear'),
        layers.Reshape(X0_tr.shape[1:])
    ])
  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

autoencoder = Autoencoder(latent_dim = 3)

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

autoencoder.compile(optimizer=opt,
              loss='mse',
              metrics=['mse'])

history = autoencoder.fit(X0_tr,
                      X0_tr,
                      epochs=150,
                      batch_size=100,
                      validation_data=(X0_tst, X0_tst),
                      verbose=0
                      )

# Summarize history for accuracy
plt.plot(history.history['mse'])
plt.plot(history.history['val_mse'])
plt.ylim(0,0.01)

X0_pred = autoencoder(X0_tst).numpy()

modelmse_tst = mean_squared_error(flat_partial(X0_tst), flat_partial(X0_pred))
print('\nOn test set \n')
print('Model mse:', modelmse_tst.round(8))

print('Comparing last element of each window: true and predicted')
plt.matshow(X0_tst[:,-1,:].T, vmin=0, vmax=1)
plt.matshow(X0_pred[:,-1,:].T, vmin=0, vmax=1)